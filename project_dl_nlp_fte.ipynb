{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import framework as fk\n",
    "import metrics as M\n",
    "import losses as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of label 0 is 99.10010010010011\n"
     ]
    }
   ],
   "source": [
    "LABEL_OF_THE_CLASS = 1\n",
    "\n",
    "def load_data(label=-1):\n",
    "    train_data_raw = pd.read_csv('data/train.csv.zip', compression='zip').values.tolist()\n",
    "    train_data = [clean_str(i[1]).split(\" \") for i in train_data_raw]\n",
    "    train_labels = [[float(j) for j in i[2:]] for i in train_data_raw]\n",
    "    \n",
    "    test_labels_raw = pd.read_csv('data/test_labels.csv.zip', compression='zip').values.tolist()\n",
    "    test_labels = [[float(j) for j in i[1:]] for i in test_labels_raw]\n",
    "    indices = [i for i in range(len(test_labels)) if test_labels[i][0] != -1]\n",
    "    test_labels = [test_labels[i] for i in indices]\n",
    "    test_data_raw = pd.read_csv('data/test.csv.zip', compression='zip').values.tolist()\n",
    "    test_data = [clean_str(i[1]).split(\" \") for i in test_data_raw]\n",
    "    test_data = [test_data[i] for i in indices]\n",
    "    \n",
    "    if label==-1:\n",
    "        train_dataset = (train_data[:100000], train_labels[:100000])\n",
    "        dev_dataset = (train_data[152000:153000], train_labels[152000:153000])\n",
    "        test_dataset = (test_data[:2000], test_labels[:2000])\n",
    "    else:\n",
    "        train_dataset = (train_data[:100000], [[i[label]] for i in train_labels[:100000]])\n",
    "        dev_dataset = (train_data[152000:153000], [[i[label]] for i in train_labels[152000:153000]])\n",
    "        test_dataset = (test_data[:2000], [[i[label]] for i in test_labels[:2000]])    \n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load_data(label=LABEL_OF_THE_CLASS)\n",
    "for j in range(len(train_dataset[1][0])):\n",
    "    weight = sum(i[j] for i in train_dataset[1])\n",
    "    print(\"Weight of label\",j,\"is\",(100000-weight)/weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim2):\n",
    "        super(LSTM_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, hidden_dim2)\n",
    "        self.linear2 = nn.Linear(hidden_dim2, 6)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        # out = self.bn1(th.relu(self.linear(self.dropout(out))))\n",
    "        out = th.relu(self.linear(self.dropout(out)))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier_single(nn.Module):\n",
    "    def __init__(self, hidden_dim, hidden_dim2, emb_weights, pad_id):\n",
    "        super(LSTM_classifier_single, self).__init__()\n",
    "        self.padding_id = pad_id\n",
    "        vocab_size = emb_weights.shape[0]\n",
    "        embedding_dim = emb_weights.shape[1]\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "        self.embedding.weight = nn.Parameter(th.tensor(emb_weights, dtype=th.float32))\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, hidden_dim2)\n",
    "        self.linear2 = nn.Linear(hidden_dim2, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        a = []\n",
    "        for i in inputs:\n",
    "            a += [max(np.where(th.LongTensor(i).numpy()==self.padding_id)[0][0],1)]\n",
    "        input_lengths = th.LongTensor(a)\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        # out = self.bn1(th.relu(self.linear(self.dropout(out))))\n",
    "        out = th.relu(self.linear(self.dropout(out)))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier_1LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim2):\n",
    "        super(LSTM_classifier_1LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.class0 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "        self.class1 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "        self.class2 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "        self.class3 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "        self.class4 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "        self.class5 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 1))\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        \n",
    "        mult_out = [model(out) for model in [self.class0, self.class1, self.class2, self.class3, self.class4, self.class5]]\n",
    "        \n",
    "        return mult_out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(M.Metric):\n",
    "    def reset(self):\n",
    "        self.totals = [0 for i in range(12)]\n",
    "        self.corrects = [0 for i in range(12)]\n",
    "    \n",
    "    def step(self, in_data, out_data, labels):\n",
    "        for i,j in zip(out_data, labels):\n",
    "            for k in range(len(j)):\n",
    "                self.totals[2*k+int(j[k])] += 1\n",
    "                if (i[2*k+1] > i[2*k]) == (j[k]==1):\n",
    "                    self.corrects[2*k+int(j[k])] += 1\n",
    "        \n",
    "    def score(self):\n",
    "        s = 0\n",
    "        t = 0\n",
    "        for i in range(12):\n",
    "            if self.totals[i] > 0:\n",
    "                t += self.corrects[i] / self.totals[i]\n",
    "                s += 1\n",
    "        return t/s\n",
    "    \n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.l = nn.CrossEntropyLoss(weight=th.Tensor([1.,99.]))\n",
    "        \n",
    "    def compute(self, in_data, out_data, labels):\n",
    "        return self.l(out_data, labels.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_dataset[0]))\n",
    "\n",
    "# Padding\n",
    "tokenizer.word_index[\"<PAD>\"] = len(tokenizer.word_index.keys())\n",
    "padding_idx = tokenizer.word_index[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset):\n",
    "    tmp = [[tokenizer.word_index[j] for j in i if j in tokenizer.word_index.keys()] for i in dataset[0]]\n",
    "    m = max(len(i) for i in tmp)\n",
    "    tmp = [i+(m-len(i)+1)*[tokenizer.word_index[\"<PAD>\"]] for i in tmp]\n",
    "    return (tmp, dataset[1])\n",
    "\n",
    "train_dataset = transform_data(train_dataset)\n",
    "dev_dataset = transform_data(dev_dataset)\n",
    "test_dataset = transform_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embedding\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def build_matrix(word_index, path, emb_size):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, emb_size))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix\n",
    "\n",
    "emb_weights = build_matrix(tokenizer.word_index, './fte_300_100', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fk.Dataset(name=\"toxic\", train_data=train_dataset[0], train_labels=train_dataset[1], \n",
    "                                   dev_data=dev_dataset[0], dev_labels=dev_dataset[1],\n",
    "                                   test_data=test_dataset[0], test_labels=test_dataset[1], \n",
    "                                   data_type='long', label_type='long', batch_size=128)\n",
    "del train_dataset; del dev_dataset; del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_classifier_single(20, 20, emb_weights, padding_idx)\n",
    "loss = Loss()\n",
    "optimizer = optim.Adagrad(model.parameters(),lr=0.1, lr_decay = 0.01)\n",
    "metric = Metric()\n",
    "\n",
    "encaps = fk.Model(name=\"lstm\", model=model, loss=loss, optimizer=optimizer, metric=metric, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/782 done - Avg. loss = tensor(4.1559e-05, grad_fn=<DivBackward0>)\n",
      "Batch 2/782 done - Avg. loss = tensor(6.9795e-05, grad_fn=<DivBackward0>)\n",
      "Batch 3/782 done - Avg. loss = tensor(6.3574e-05, grad_fn=<DivBackward0>)\n",
      "Batch 4/782 done - Avg. loss = tensor(6.2298e-05, grad_fn=<DivBackward0>)\n",
      "Batch 5/782 done - Avg. loss = tensor(5.9775e-05, grad_fn=<DivBackward0>)\n",
      "Batch 6/782 done - Avg. loss = tensor(5.6810e-05, grad_fn=<DivBackward0>)\n",
      "Batch 7/782 done - Avg. loss = tensor(5.3790e-05, grad_fn=<DivBackward0>)\n",
      "Batch 8/782 done - Avg. loss = tensor(5.1164e-05, grad_fn=<DivBackward0>)\n",
      "Batch 9/782 done - Avg. loss = tensor(4.9047e-05, grad_fn=<DivBackward0>)\n",
      "Batch 10/782 done - Avg. loss = tensor(5.8879e-05, grad_fn=<DivBackward0>)\n",
      "Batch 11/782 done - Avg. loss = tensor(5.7022e-05, grad_fn=<DivBackward0>)\n",
      "Batch 12/782 done - Avg. loss = tensor(5.5005e-05, grad_fn=<DivBackward0>)\n",
      "Batch 13/782 done - Avg. loss = tensor(5.3416e-05, grad_fn=<DivBackward0>)\n",
      "Batch 14/782 done - Avg. loss = tensor(5.1171e-05, grad_fn=<DivBackward0>)\n",
      "Batch 15/782 done - Avg. loss = tensor(4.9140e-05, grad_fn=<DivBackward0>)\n",
      "Batch 16/782 done - Avg. loss = tensor(5.0950e-05, grad_fn=<DivBackward0>)\n",
      "Batch 17/782 done - Avg. loss = tensor(4.9722e-05, grad_fn=<DivBackward0>)\n",
      "Batch 18/782 done - Avg. loss = tensor(4.8009e-05, grad_fn=<DivBackward0>)\n",
      "Batch 19/782 done - Avg. loss = tensor(4.7509e-05, grad_fn=<DivBackward0>)\n",
      "Batch 20/782 done - Avg. loss = tensor(4.5978e-05, grad_fn=<DivBackward0>)\n",
      "Batch 21/782 done - Avg. loss = tensor(4.4369e-05, grad_fn=<DivBackward0>)\n",
      "Batch 22/782 done - Avg. loss = tensor(4.3341e-05, grad_fn=<DivBackward0>)\n",
      "Batch 23/782 done - Avg. loss = tensor(4.2016e-05, grad_fn=<DivBackward0>)\n",
      "Batch 24/782 done - Avg. loss = tensor(4.2160e-05, grad_fn=<DivBackward0>)\n",
      "Batch 25/782 done - Avg. loss = tensor(4.1029e-05, grad_fn=<DivBackward0>)\n",
      "Batch 26/782 done - Avg. loss = tensor(4.0206e-05, grad_fn=<DivBackward0>)\n",
      "Batch 27/782 done - Avg. loss = tensor(4.0654e-05, grad_fn=<DivBackward0>)\n",
      "Batch 28/782 done - Avg. loss = tensor(3.9618e-05, grad_fn=<DivBackward0>)\n",
      "Batch 29/782 done - Avg. loss = tensor(3.8741e-05, grad_fn=<DivBackward0>)\n",
      "Batch 30/782 done - Avg. loss = tensor(4.0018e-05, grad_fn=<DivBackward0>)\n",
      "Batch 31/782 done - Avg. loss = tensor(3.9129e-05, grad_fn=<DivBackward0>)\n",
      "Batch 32/782 done - Avg. loss = tensor(3.8281e-05, grad_fn=<DivBackward0>)\n",
      "Batch 33/782 done - Avg. loss = tensor(3.7413e-05, grad_fn=<DivBackward0>)\n",
      "Batch 34/782 done - Avg. loss = tensor(3.6831e-05, grad_fn=<DivBackward0>)\n",
      "Batch 35/782 done - Avg. loss = tensor(3.6643e-05, grad_fn=<DivBackward0>)\n",
      "Batch 36/782 done - Avg. loss = tensor(3.6036e-05, grad_fn=<DivBackward0>)\n",
      "Batch 37/782 done - Avg. loss = tensor(3.6341e-05, grad_fn=<DivBackward0>)\n",
      "Batch 38/782 done - Avg. loss = tensor(3.5780e-05, grad_fn=<DivBackward0>)\n",
      "Batch 39/782 done - Avg. loss = tensor(3.5352e-05, grad_fn=<DivBackward0>)\n",
      "Batch 40/782 done - Avg. loss = tensor(3.4903e-05, grad_fn=<DivBackward0>)\n",
      "Batch 41/782 done - Avg. loss = tensor(3.4531e-05, grad_fn=<DivBackward0>)\n",
      "Batch 42/782 done - Avg. loss = tensor(3.4036e-05, grad_fn=<DivBackward0>)\n",
      "Batch 43/782 done - Avg. loss = tensor(3.4664e-05, grad_fn=<DivBackward0>)\n",
      "Batch 44/782 done - Avg. loss = tensor(3.4500e-05, grad_fn=<DivBackward0>)\n",
      "Batch 45/782 done - Avg. loss = tensor(3.4009e-05, grad_fn=<DivBackward0>)\n",
      "Batch 46/782 done - Avg. loss = tensor(3.3692e-05, grad_fn=<DivBackward0>)\n",
      "Batch 47/782 done - Avg. loss = tensor(3.3068e-05, grad_fn=<DivBackward0>)\n",
      "Batch 48/782 done - Avg. loss = tensor(3.2563e-05, grad_fn=<DivBackward0>)\n",
      "Batch 49/782 done - Avg. loss = tensor(3.2033e-05, grad_fn=<DivBackward0>)\n",
      "Batch 50/782 done - Avg. loss = tensor(3.2319e-05, grad_fn=<DivBackward0>)\n",
      "Batch 51/782 done - Avg. loss = tensor(3.3700e-05, grad_fn=<DivBackward0>)\n",
      "Batch 52/782 done - Avg. loss = tensor(3.3474e-05, grad_fn=<DivBackward0>)\n",
      "Batch 53/782 done - Avg. loss = tensor(3.3254e-05, grad_fn=<DivBackward0>)\n",
      "Batch 54/782 done - Avg. loss = tensor(3.3045e-05, grad_fn=<DivBackward0>)\n",
      "Batch 55/782 done - Avg. loss = tensor(3.2728e-05, grad_fn=<DivBackward0>)\n",
      "Batch 56/782 done - Avg. loss = tensor(3.3181e-05, grad_fn=<DivBackward0>)\n",
      "Batch 57/782 done - Avg. loss = tensor(3.2942e-05, grad_fn=<DivBackward0>)\n",
      "Batch 58/782 done - Avg. loss = tensor(3.2598e-05, grad_fn=<DivBackward0>)\n",
      "Batch 59/782 done - Avg. loss = tensor(3.2227e-05, grad_fn=<DivBackward0>)\n",
      "Batch 60/782 done - Avg. loss = tensor(3.2136e-05, grad_fn=<DivBackward0>)\n",
      "Batch 61/782 done - Avg. loss = tensor(3.2014e-05, grad_fn=<DivBackward0>)\n",
      "Batch 62/782 done - Avg. loss = tensor(3.1771e-05, grad_fn=<DivBackward0>)\n",
      "Batch 63/782 done - Avg. loss = tensor(3.2156e-05, grad_fn=<DivBackward0>)\n",
      "Batch 64/782 done - Avg. loss = tensor(3.1917e-05, grad_fn=<DivBackward0>)\n",
      "Batch 65/782 done - Avg. loss = tensor(3.2364e-05, grad_fn=<DivBackward0>)\n",
      "Batch 66/782 done - Avg. loss = tensor(3.2145e-05, grad_fn=<DivBackward0>)\n",
      "Batch 67/782 done - Avg. loss = tensor(3.1804e-05, grad_fn=<DivBackward0>)\n",
      "Batch 68/782 done - Avg. loss = tensor(3.1501e-05, grad_fn=<DivBackward0>)\n",
      "Batch 69/782 done - Avg. loss = tensor(3.1172e-05, grad_fn=<DivBackward0>)\n",
      "Batch 70/782 done - Avg. loss = tensor(3.1757e-05, grad_fn=<DivBackward0>)\n",
      "Batch 71/782 done - Avg. loss = tensor(3.1528e-05, grad_fn=<DivBackward0>)\n",
      "Batch 72/782 done - Avg. loss = tensor(3.1268e-05, grad_fn=<DivBackward0>)\n",
      "Batch 73/782 done - Avg. loss = tensor(3.0966e-05, grad_fn=<DivBackward0>)\n",
      "Batch 74/782 done - Avg. loss = tensor(3.0683e-05, grad_fn=<DivBackward0>)\n",
      "Batch 75/782 done - Avg. loss = tensor(3.0402e-05, grad_fn=<DivBackward0>)\n",
      "Batch 76/782 done - Avg. loss = tensor(3.0083e-05, grad_fn=<DivBackward0>)\n",
      "Batch 77/782 done - Avg. loss = tensor(2.9791e-05, grad_fn=<DivBackward0>)\n",
      "Batch 78/782 done - Avg. loss = tensor(2.9500e-05, grad_fn=<DivBackward0>)\n",
      "Batch 79/782 done - Avg. loss = tensor(2.9169e-05, grad_fn=<DivBackward0>)\n",
      "Batch 80/782 done - Avg. loss = tensor(2.9027e-05, grad_fn=<DivBackward0>)\n",
      "Batch 81/782 done - Avg. loss = tensor(2.8767e-05, grad_fn=<DivBackward0>)\n",
      "Batch 82/782 done - Avg. loss = tensor(2.8507e-05, grad_fn=<DivBackward0>)\n",
      "Batch 83/782 done - Avg. loss = tensor(2.8266e-05, grad_fn=<DivBackward0>)\n",
      "Batch 84/782 done - Avg. loss = tensor(2.8010e-05, grad_fn=<DivBackward0>)\n",
      "Batch 85/782 done - Avg. loss = tensor(2.7881e-05, grad_fn=<DivBackward0>)\n",
      "Batch 86/782 done - Avg. loss = tensor(2.8903e-05, grad_fn=<DivBackward0>)\n",
      "Batch 87/782 done - Avg. loss = tensor(2.8650e-05, grad_fn=<DivBackward0>)\n",
      "Batch 88/782 done - Avg. loss = tensor(2.8429e-05, grad_fn=<DivBackward0>)\n",
      "Batch 89/782 done - Avg. loss = tensor(2.8728e-05, grad_fn=<DivBackward0>)\n",
      "Batch 90/782 done - Avg. loss = tensor(2.8615e-05, grad_fn=<DivBackward0>)\n",
      "Batch 91/782 done - Avg. loss = tensor(2.8623e-05, grad_fn=<DivBackward0>)\n",
      "Batch 92/782 done - Avg. loss = tensor(2.8464e-05, grad_fn=<DivBackward0>)\n",
      "Batch 93/782 done - Avg. loss = tensor(2.8250e-05, grad_fn=<DivBackward0>)\n",
      "Batch 94/782 done - Avg. loss = tensor(2.8074e-05, grad_fn=<DivBackward0>)\n",
      "Batch 95/782 done - Avg. loss = tensor(2.7871e-05, grad_fn=<DivBackward0>)\n",
      "Batch 96/782 done - Avg. loss = tensor(2.7838e-05, grad_fn=<DivBackward0>)\n",
      "Batch 97/782 done - Avg. loss = tensor(2.7863e-05, grad_fn=<DivBackward0>)\n",
      "Batch 98/782 done - Avg. loss = tensor(2.7640e-05, grad_fn=<DivBackward0>)\n",
      "Batch 99/782 done - Avg. loss = tensor(2.7509e-05, grad_fn=<DivBackward0>)\n",
      "Batch 100/782 done - Avg. loss = tensor(2.7569e-05, grad_fn=<DivBackward0>)\n",
      "Batch 101/782 done - Avg. loss = tensor(2.7398e-05, grad_fn=<DivBackward0>)\n",
      "Batch 102/782 done - Avg. loss = tensor(2.7303e-05, grad_fn=<DivBackward0>)\n",
      "Batch 103/782 done - Avg. loss = tensor(2.7167e-05, grad_fn=<DivBackward0>)\n",
      "Batch 104/782 done - Avg. loss = tensor(2.6997e-05, grad_fn=<DivBackward0>)\n",
      "Batch 105/782 done - Avg. loss = tensor(2.6918e-05, grad_fn=<DivBackward0>)\n",
      "Batch 106/782 done - Avg. loss = tensor(2.7033e-05, grad_fn=<DivBackward0>)\n",
      "Batch 107/782 done - Avg. loss = tensor(2.6843e-05, grad_fn=<DivBackward0>)\n",
      "Batch 108/782 done - Avg. loss = tensor(2.6670e-05, grad_fn=<DivBackward0>)\n",
      "Batch 109/782 done - Avg. loss = tensor(2.6514e-05, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 110/782 done - Avg. loss = tensor(2.6326e-05, grad_fn=<DivBackward0>)\n",
      "Batch 111/782 done - Avg. loss = tensor(2.6125e-05, grad_fn=<DivBackward0>)\n",
      "Batch 112/782 done - Avg. loss = tensor(2.6034e-05, grad_fn=<DivBackward0>)\n",
      "Batch 113/782 done - Avg. loss = tensor(2.6371e-05, grad_fn=<DivBackward0>)\n",
      "Batch 114/782 done - Avg. loss = tensor(2.6820e-05, grad_fn=<DivBackward0>)\n",
      "Batch 115/782 done - Avg. loss = tensor(2.6692e-05, grad_fn=<DivBackward0>)\n",
      "Batch 116/782 done - Avg. loss = tensor(2.6537e-05, grad_fn=<DivBackward0>)\n",
      "Batch 117/782 done - Avg. loss = tensor(2.6369e-05, grad_fn=<DivBackward0>)\n",
      "Batch 118/782 done - Avg. loss = tensor(2.6200e-05, grad_fn=<DivBackward0>)\n",
      "Batch 119/782 done - Avg. loss = tensor(2.6042e-05, grad_fn=<DivBackward0>)\n",
      "Batch 120/782 done - Avg. loss = tensor(2.6198e-05, grad_fn=<DivBackward0>)\n",
      "Batch 121/782 done - Avg. loss = tensor(2.6065e-05, grad_fn=<DivBackward0>)\n",
      "Batch 122/782 done - Avg. loss = tensor(2.5940e-05, grad_fn=<DivBackward0>)\n",
      "Batch 123/782 done - Avg. loss = tensor(2.6064e-05, grad_fn=<DivBackward0>)\n",
      "Batch 124/782 done - Avg. loss = tensor(2.5927e-05, grad_fn=<DivBackward0>)\n",
      "Batch 125/782 done - Avg. loss = tensor(2.5811e-05, grad_fn=<DivBackward0>)\n",
      "Batch 126/782 done - Avg. loss = tensor(2.5658e-05, grad_fn=<DivBackward0>)\n",
      "Batch 127/782 done - Avg. loss = tensor(2.5575e-05, grad_fn=<DivBackward0>)\n",
      "Batch 128/782 done - Avg. loss = tensor(2.5621e-05, grad_fn=<DivBackward0>)\n",
      "Batch 129/782 done - Avg. loss = tensor(2.5502e-05, grad_fn=<DivBackward0>)\n",
      "Batch 130/782 done - Avg. loss = tensor(2.5373e-05, grad_fn=<DivBackward0>)\n",
      "Batch 131/782 done - Avg. loss = tensor(2.5271e-05, grad_fn=<DivBackward0>)\n",
      "Batch 132/782 done - Avg. loss = tensor(2.5587e-05, grad_fn=<DivBackward0>)\n",
      "Batch 133/782 done - Avg. loss = tensor(2.5460e-05, grad_fn=<DivBackward0>)\n",
      "Batch 134/782 done - Avg. loss = tensor(2.5328e-05, grad_fn=<DivBackward0>)\n",
      "Batch 135/782 done - Avg. loss = tensor(2.5217e-05, grad_fn=<DivBackward0>)\n",
      "Batch 136/782 done - Avg. loss = tensor(2.5108e-05, grad_fn=<DivBackward0>)\n",
      "Batch 137/782 done - Avg. loss = tensor(2.5022e-05, grad_fn=<DivBackward0>)\n",
      "Batch 138/782 done - Avg. loss = tensor(2.4932e-05, grad_fn=<DivBackward0>)\n",
      "Batch 139/782 done - Avg. loss = tensor(2.5168e-05, grad_fn=<DivBackward0>)\n",
      "Batch 140/782 done - Avg. loss = tensor(2.5082e-05, grad_fn=<DivBackward0>)\n",
      "Batch 141/782 done - Avg. loss = tensor(2.4996e-05, grad_fn=<DivBackward0>)\n",
      "Batch 142/782 done - Avg. loss = tensor(2.5092e-05, grad_fn=<DivBackward0>)\n",
      "Batch 143/782 done - Avg. loss = tensor(2.4994e-05, grad_fn=<DivBackward0>)\n",
      "Batch 144/782 done - Avg. loss = tensor(2.4874e-05, grad_fn=<DivBackward0>)\n",
      "Batch 145/782 done - Avg. loss = tensor(2.4769e-05, grad_fn=<DivBackward0>)\n",
      "Batch 146/782 done - Avg. loss = tensor(2.4995e-05, grad_fn=<DivBackward0>)\n",
      "Batch 147/782 done - Avg. loss = tensor(2.4900e-05, grad_fn=<DivBackward0>)\n",
      "Batch 148/782 done - Avg. loss = tensor(2.4967e-05, grad_fn=<DivBackward0>)\n",
      "Batch 149/782 done - Avg. loss = tensor(2.4941e-05, grad_fn=<DivBackward0>)\n",
      "Batch 150/782 done - Avg. loss = tensor(2.4849e-05, grad_fn=<DivBackward0>)\n",
      "Batch 151/782 done - Avg. loss = tensor(2.4732e-05, grad_fn=<DivBackward0>)\n",
      "Batch 152/782 done - Avg. loss = tensor(2.4834e-05, grad_fn=<DivBackward0>)\n",
      "Batch 153/782 done - Avg. loss = tensor(2.4756e-05, grad_fn=<DivBackward0>)\n",
      "Batch 154/782 done - Avg. loss = tensor(2.4703e-05, grad_fn=<DivBackward0>)\n",
      "Batch 155/782 done - Avg. loss = tensor(2.4679e-05, grad_fn=<DivBackward0>)\n",
      "Batch 156/782 done - Avg. loss = tensor(2.4573e-05, grad_fn=<DivBackward0>)\n",
      "Batch 157/782 done - Avg. loss = tensor(2.4470e-05, grad_fn=<DivBackward0>)\n",
      "Batch 158/782 done - Avg. loss = tensor(2.4356e-05, grad_fn=<DivBackward0>)\n",
      "Batch 159/782 done - Avg. loss = tensor(2.4245e-05, grad_fn=<DivBackward0>)\n",
      "Batch 160/782 done - Avg. loss = tensor(2.4144e-05, grad_fn=<DivBackward0>)\n",
      "Batch 161/782 done - Avg. loss = tensor(2.4046e-05, grad_fn=<DivBackward0>)\n",
      "Batch 162/782 done - Avg. loss = tensor(2.3930e-05, grad_fn=<DivBackward0>)\n",
      "Batch 163/782 done - Avg. loss = tensor(2.3850e-05, grad_fn=<DivBackward0>)\n",
      "Batch 164/782 done - Avg. loss = tensor(2.3757e-05, grad_fn=<DivBackward0>)\n",
      "Batch 165/782 done - Avg. loss = tensor(2.3884e-05, grad_fn=<DivBackward0>)\n",
      "Batch 166/782 done - Avg. loss = tensor(2.3785e-05, grad_fn=<DivBackward0>)\n",
      "Batch 167/782 done - Avg. loss = tensor(2.3681e-05, grad_fn=<DivBackward0>)\n",
      "Batch 168/782 done - Avg. loss = tensor(2.3632e-05, grad_fn=<DivBackward0>)\n",
      "Batch 169/782 done - Avg. loss = tensor(2.3667e-05, grad_fn=<DivBackward0>)\n",
      "Batch 170/782 done - Avg. loss = tensor(2.3616e-05, grad_fn=<DivBackward0>)\n",
      "Batch 171/782 done - Avg. loss = tensor(2.3641e-05, grad_fn=<DivBackward0>)\n",
      "Batch 172/782 done - Avg. loss = tensor(2.3793e-05, grad_fn=<DivBackward0>)\n",
      "Batch 173/782 done - Avg. loss = tensor(2.3772e-05, grad_fn=<DivBackward0>)\n",
      "Batch 174/782 done - Avg. loss = tensor(2.3680e-05, grad_fn=<DivBackward0>)\n",
      "Batch 175/782 done - Avg. loss = tensor(2.3658e-05, grad_fn=<DivBackward0>)\n",
      "Batch 176/782 done - Avg. loss = tensor(2.3598e-05, grad_fn=<DivBackward0>)\n",
      "Batch 177/782 done - Avg. loss = tensor(2.3531e-05, grad_fn=<DivBackward0>)\n",
      "Batch 178/782 done - Avg. loss = tensor(2.3815e-05, grad_fn=<DivBackward0>)\n",
      "Batch 179/782 done - Avg. loss = tensor(2.3760e-05, grad_fn=<DivBackward0>)\n",
      "Batch 180/782 done - Avg. loss = tensor(2.3657e-05, grad_fn=<DivBackward0>)\n",
      "Batch 181/782 done - Avg. loss = tensor(2.3565e-05, grad_fn=<DivBackward0>)\n",
      "Batch 182/782 done - Avg. loss = tensor(2.3481e-05, grad_fn=<DivBackward0>)\n",
      "Batch 183/782 done - Avg. loss = tensor(2.3576e-05, grad_fn=<DivBackward0>)\n",
      "Batch 184/782 done - Avg. loss = tensor(2.3489e-05, grad_fn=<DivBackward0>)\n",
      "Batch 185/782 done - Avg. loss = tensor(2.3412e-05, grad_fn=<DivBackward0>)\n",
      "Batch 186/782 done - Avg. loss = tensor(2.3321e-05, grad_fn=<DivBackward0>)\n",
      "Batch 187/782 done - Avg. loss = tensor(2.3235e-05, grad_fn=<DivBackward0>)\n",
      "Batch 188/782 done - Avg. loss = tensor(2.3167e-05, grad_fn=<DivBackward0>)\n",
      "Batch 189/782 done - Avg. loss = tensor(2.3168e-05, grad_fn=<DivBackward0>)\n",
      "Batch 190/782 done - Avg. loss = tensor(2.3116e-05, grad_fn=<DivBackward0>)\n",
      "Batch 191/782 done - Avg. loss = tensor(2.3065e-05, grad_fn=<DivBackward0>)\n",
      "Batch 192/782 done - Avg. loss = tensor(2.3004e-05, grad_fn=<DivBackward0>)\n",
      "Batch 193/782 done - Avg. loss = tensor(2.2971e-05, grad_fn=<DivBackward0>)\n",
      "Batch 194/782 done - Avg. loss = tensor(2.3089e-05, grad_fn=<DivBackward0>)\n",
      "Batch 195/782 done - Avg. loss = tensor(2.3066e-05, grad_fn=<DivBackward0>)\n",
      "Batch 196/782 done - Avg. loss = tensor(2.3229e-05, grad_fn=<DivBackward0>)\n",
      "Batch 197/782 done - Avg. loss = tensor(2.3186e-05, grad_fn=<DivBackward0>)\n",
      "Batch 198/782 done - Avg. loss = tensor(2.3102e-05, grad_fn=<DivBackward0>)\n",
      "Batch 199/782 done - Avg. loss = tensor(2.3047e-05, grad_fn=<DivBackward0>)\n",
      "Batch 200/782 done - Avg. loss = tensor(2.2960e-05, grad_fn=<DivBackward0>)\n",
      "Batch 201/782 done - Avg. loss = tensor(2.2907e-05, grad_fn=<DivBackward0>)\n",
      "Batch 202/782 done - Avg. loss = tensor(2.2827e-05, grad_fn=<DivBackward0>)\n",
      "Batch 203/782 done - Avg. loss = tensor(2.2788e-05, grad_fn=<DivBackward0>)\n",
      "Batch 204/782 done - Avg. loss = tensor(2.2757e-05, grad_fn=<DivBackward0>)\n",
      "Batch 205/782 done - Avg. loss = tensor(2.2680e-05, grad_fn=<DivBackward0>)\n",
      "Batch 206/782 done - Avg. loss = tensor(2.2612e-05, grad_fn=<DivBackward0>)\n",
      "Batch 207/782 done - Avg. loss = tensor(2.2640e-05, grad_fn=<DivBackward0>)\n",
      "Batch 208/782 done - Avg. loss = tensor(2.2622e-05, grad_fn=<DivBackward0>)\n",
      "Batch 209/782 done - Avg. loss = tensor(2.2647e-05, grad_fn=<DivBackward0>)\n",
      "Batch 210/782 done - Avg. loss = tensor(2.2562e-05, grad_fn=<DivBackward0>)\n",
      "Batch 211/782 done - Avg. loss = tensor(2.2509e-05, grad_fn=<DivBackward0>)\n",
      "Batch 212/782 done - Avg. loss = tensor(2.2438e-05, grad_fn=<DivBackward0>)\n",
      "Batch 213/782 done - Avg. loss = tensor(2.2366e-05, grad_fn=<DivBackward0>)\n",
      "Batch 214/782 done - Avg. loss = tensor(2.2302e-05, grad_fn=<DivBackward0>)\n",
      "Batch 215/782 done - Avg. loss = tensor(2.2234e-05, grad_fn=<DivBackward0>)\n",
      "Batch 216/782 done - Avg. loss = tensor(2.2198e-05, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 217/782 done - Avg. loss = tensor(2.2280e-05, grad_fn=<DivBackward0>)\n",
      "Batch 218/782 done - Avg. loss = tensor(2.2244e-05, grad_fn=<DivBackward0>)\n",
      "Batch 219/782 done - Avg. loss = tensor(2.2195e-05, grad_fn=<DivBackward0>)\n",
      "Batch 220/782 done - Avg. loss = tensor(2.2158e-05, grad_fn=<DivBackward0>)\n",
      "Batch 221/782 done - Avg. loss = tensor(2.2078e-05, grad_fn=<DivBackward0>)\n",
      "Batch 222/782 done - Avg. loss = tensor(2.2017e-05, grad_fn=<DivBackward0>)\n",
      "Batch 223/782 done - Avg. loss = tensor(2.1973e-05, grad_fn=<DivBackward0>)\n",
      "Batch 224/782 done - Avg. loss = tensor(2.1943e-05, grad_fn=<DivBackward0>)\n",
      "Batch 225/782 done - Avg. loss = tensor(2.1882e-05, grad_fn=<DivBackward0>)\n",
      "Batch 226/782 done - Avg. loss = tensor(2.1828e-05, grad_fn=<DivBackward0>)\n",
      "Batch 227/782 done - Avg. loss = tensor(2.1886e-05, grad_fn=<DivBackward0>)\n",
      "Batch 228/782 done - Avg. loss = tensor(2.1834e-05, grad_fn=<DivBackward0>)\n",
      "Batch 229/782 done - Avg. loss = tensor(2.1823e-05, grad_fn=<DivBackward0>)\n",
      "Batch 230/782 done - Avg. loss = tensor(2.1790e-05, grad_fn=<DivBackward0>)\n",
      "Batch 231/782 done - Avg. loss = tensor(2.1744e-05, grad_fn=<DivBackward0>)\n",
      "Batch 232/782 done - Avg. loss = tensor(2.1672e-05, grad_fn=<DivBackward0>)\n",
      "Batch 233/782 done - Avg. loss = tensor(2.1643e-05, grad_fn=<DivBackward0>)\n",
      "Batch 234/782 done - Avg. loss = tensor(2.1569e-05, grad_fn=<DivBackward0>)\n",
      "Batch 235/782 done - Avg. loss = tensor(2.1628e-05, grad_fn=<DivBackward0>)\n",
      "Batch 236/782 done - Avg. loss = tensor(2.1571e-05, grad_fn=<DivBackward0>)\n",
      "Batch 237/782 done - Avg. loss = tensor(2.1572e-05, grad_fn=<DivBackward0>)\n",
      "Batch 238/782 done - Avg. loss = tensor(2.1509e-05, grad_fn=<DivBackward0>)\n",
      "Batch 239/782 done - Avg. loss = tensor(2.1438e-05, grad_fn=<DivBackward0>)\n",
      "Batch 240/782 done - Avg. loss = tensor(2.1395e-05, grad_fn=<DivBackward0>)\n",
      "Batch 241/782 done - Avg. loss = tensor(2.1333e-05, grad_fn=<DivBackward0>)\n",
      "Batch 242/782 done - Avg. loss = tensor(2.1289e-05, grad_fn=<DivBackward0>)\n",
      "Batch 243/782 done - Avg. loss = tensor(2.1234e-05, grad_fn=<DivBackward0>)\n",
      "Batch 244/782 done - Avg. loss = tensor(2.1162e-05, grad_fn=<DivBackward0>)\n",
      "Batch 245/782 done - Avg. loss = tensor(2.1099e-05, grad_fn=<DivBackward0>)\n",
      "Batch 246/782 done - Avg. loss = tensor(2.1038e-05, grad_fn=<DivBackward0>)\n",
      "Batch 247/782 done - Avg. loss = tensor(2.0998e-05, grad_fn=<DivBackward0>)\n",
      "Batch 248/782 done - Avg. loss = tensor(2.0992e-05, grad_fn=<DivBackward0>)\n",
      "Batch 249/782 done - Avg. loss = tensor(2.0936e-05, grad_fn=<DivBackward0>)\n",
      "Batch 250/782 done - Avg. loss = tensor(2.0892e-05, grad_fn=<DivBackward0>)\n",
      "Batch 251/782 done - Avg. loss = tensor(2.0863e-05, grad_fn=<DivBackward0>)\n",
      "Batch 252/782 done - Avg. loss = tensor(2.0807e-05, grad_fn=<DivBackward0>)\n",
      "Batch 253/782 done - Avg. loss = tensor(2.0744e-05, grad_fn=<DivBackward0>)\n",
      "Batch 254/782 done - Avg. loss = tensor(2.0687e-05, grad_fn=<DivBackward0>)\n",
      "Batch 255/782 done - Avg. loss = tensor(2.0631e-05, grad_fn=<DivBackward0>)\n",
      "Batch 256/782 done - Avg. loss = tensor(2.0559e-05, grad_fn=<DivBackward0>)\n",
      "Batch 257/782 done - Avg. loss = tensor(2.0512e-05, grad_fn=<DivBackward0>)\n",
      "Batch 258/782 done - Avg. loss = tensor(2.0451e-05, grad_fn=<DivBackward0>)\n",
      "Batch 259/782 done - Avg. loss = tensor(2.0462e-05, grad_fn=<DivBackward0>)\n",
      "Batch 260/782 done - Avg. loss = tensor(2.0405e-05, grad_fn=<DivBackward0>)\n",
      "Batch 261/782 done - Avg. loss = tensor(2.0344e-05, grad_fn=<DivBackward0>)\n",
      "Batch 262/782 done - Avg. loss = tensor(2.0481e-05, grad_fn=<DivBackward0>)\n",
      "Batch 263/782 done - Avg. loss = tensor(2.0439e-05, grad_fn=<DivBackward0>)\n",
      "Batch 264/782 done - Avg. loss = tensor(2.0379e-05, grad_fn=<DivBackward0>)\n",
      "Batch 265/782 done - Avg. loss = tensor(2.0324e-05, grad_fn=<DivBackward0>)\n",
      "Batch 266/782 done - Avg. loss = tensor(2.0270e-05, grad_fn=<DivBackward0>)\n",
      "Batch 267/782 done - Avg. loss = tensor(2.0381e-05, grad_fn=<DivBackward0>)\n",
      "Batch 268/782 done - Avg. loss = tensor(2.0326e-05, grad_fn=<DivBackward0>)\n",
      "Batch 269/782 done - Avg. loss = tensor(2.0274e-05, grad_fn=<DivBackward0>)\n",
      "Batch 270/782 done - Avg. loss = tensor(2.0244e-05, grad_fn=<DivBackward0>)\n",
      "Batch 271/782 done - Avg. loss = tensor(2.0194e-05, grad_fn=<DivBackward0>)\n",
      "Batch 272/782 done - Avg. loss = tensor(2.0569e-05, grad_fn=<DivBackward0>)\n",
      "Batch 273/782 done - Avg. loss = tensor(2.0828e-05, grad_fn=<DivBackward0>)\n",
      "Batch 274/782 done - Avg. loss = tensor(2.0780e-05, grad_fn=<DivBackward0>)\n",
      "Batch 275/782 done - Avg. loss = tensor(2.0768e-05, grad_fn=<DivBackward0>)\n",
      "Batch 276/782 done - Avg. loss = tensor(2.0733e-05, grad_fn=<DivBackward0>)\n",
      "Batch 277/782 done - Avg. loss = tensor(2.0680e-05, grad_fn=<DivBackward0>)\n",
      "Batch 278/782 done - Avg. loss = tensor(2.0632e-05, grad_fn=<DivBackward0>)\n",
      "Batch 279/782 done - Avg. loss = tensor(2.0580e-05, grad_fn=<DivBackward0>)\n",
      "Batch 280/782 done - Avg. loss = tensor(2.0524e-05, grad_fn=<DivBackward0>)\n",
      "Batch 281/782 done - Avg. loss = tensor(2.0593e-05, grad_fn=<DivBackward0>)\n",
      "Batch 282/782 done - Avg. loss = tensor(2.0550e-05, grad_fn=<DivBackward0>)\n",
      "Batch 283/782 done - Avg. loss = tensor(2.0511e-05, grad_fn=<DivBackward0>)\n",
      "Batch 284/782 done - Avg. loss = tensor(2.0499e-05, grad_fn=<DivBackward0>)\n",
      "Batch 285/782 done - Avg. loss = tensor(2.0455e-05, grad_fn=<DivBackward0>)\n",
      "Batch 286/782 done - Avg. loss = tensor(2.0406e-05, grad_fn=<DivBackward0>)\n",
      "Batch 287/782 done - Avg. loss = tensor(2.0441e-05, grad_fn=<DivBackward0>)\n",
      "Batch 288/782 done - Avg. loss = tensor(2.0444e-05, grad_fn=<DivBackward0>)\n",
      "Batch 289/782 done - Avg. loss = tensor(2.0406e-05, grad_fn=<DivBackward0>)\n",
      "Batch 290/782 done - Avg. loss = tensor(2.0364e-05, grad_fn=<DivBackward0>)\n",
      "Batch 291/782 done - Avg. loss = tensor(2.0323e-05, grad_fn=<DivBackward0>)\n",
      "Batch 292/782 done - Avg. loss = tensor(2.0287e-05, grad_fn=<DivBackward0>)\n",
      "Batch 293/782 done - Avg. loss = tensor(2.0273e-05, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fk.device = th.device(\"cpu\")\n",
    "encaps.train(epochs=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric score is : 0.918404504898178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.918404504898178"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encaps.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.918"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
