{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import framework as fk\n",
    "import metrics as M\n",
    "import losses as L\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim2):\n",
    "        super(LSTM_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, hidden_dim2)\n",
    "        self.linear2 = nn.Linear(hidden_dim2, 6)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        a = []\n",
    "        for i in inputs:\n",
    "            a += [max(np.where(th.LongTensor(i).numpy()==identifiers[\"<PAD>\"])[0][0],1)]\n",
    "        input_lengths = th.LongTensor(a)\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        # out = self.bn1(th.relu(self.linear(self.dropout(out))))\n",
    "        out = th.relu(self.linear(self.dropout(out)))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier_single(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim2):\n",
    "        super(LSTM_classifier_single, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, hidden_dim2)\n",
    "        self.linear2 = nn.Linear(hidden_dim2, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        a = []\n",
    "        for i in inputs:\n",
    "            a += [max(np.where(th.LongTensor(i).numpy()==identifiers[\"<PAD>\"])[0][0],1)]\n",
    "        input_lengths = th.LongTensor(a)\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        # out = self.bn1(th.relu(self.linear(self.dropout(out))))\n",
    "        out = th.relu(self.linear(self.dropout(out)))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier_1LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim2):\n",
    "        super(LSTM_classifier_1LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.class0 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "        self.class1 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "        self.class2 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "        self.class3 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "        self.class4 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "        self.class5 = nn.Sequential(nn.Dropout(0.2), nn.Linear(2*hidden_dim, hidden_dim2), nn.Linear(hidden_dim2, 2))\n",
    "    \n",
    "    def last_timestep(self, unpacked, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
    "                                               unpacked.size(2)).unsqueeze(1)\n",
    "        return unpacked.gather(1, idx).squeeze()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        a = []\n",
    "        for i in inputs:\n",
    "            a += [max(np.where(th.LongTensor(i).numpy()==identifiers[\"<PAD>\"])[0][0],1)]\n",
    "        input_lengths = th.LongTensor(a)\n",
    "        out = self.embedding(inputs)\n",
    "        out = th.nn.utils.rnn.pack_padded_sequence(out, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        print(out)\n",
    "        out, (h0,h1) = self.lstm(out)\n",
    "        print(out)\n",
    "        out, _ = th.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        print(out)\n",
    "        out = self.last_timestep(out, input_lengths)\n",
    "        \n",
    "        mult_out = [model(out) for model in [self.class0, self.class1, self.class2, self.class3, self.class4, self.class5]]\n",
    "        \n",
    "        return mult_out\n",
    "    \n",
    "    def make_input(self, sentences):\n",
    "        X_lengths = [len(sentence) for sentence in sentences]\n",
    "        \n",
    "        padding_token = identifiers['<PAD>']\n",
    "        \n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(sentences)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * padding_token\n",
    "        \n",
    "        # copy over the actual sequences\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = sentences[i]\n",
    "            padded_X[i, 0:len(sequence)] = sequence\n",
    "    \n",
    "        return th.from_numpy(padded_X).long(), th.from_numpy(np.array([len(sentence) for sentence in sentences])).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our computations\n",
    "We load our data, create our dataset and our model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the data and returns the values for the label we specify\n",
    "# -1 means all the labels are returned\n",
    "def load_data(label=-1):\n",
    "    # We load the train dataset\n",
    "    train_data_raw = pd.read_csv('data/train.csv.zip', compression='zip').values.tolist()\n",
    "    train_data = [clean_str(i[1]).split(\" \") for i in train_data_raw]\n",
    "    train_labels = [[float(j) for j in i[2:]] for i in train_data_raw]\n",
    "    \n",
    "    # We load the test dataset and keep only the examples with valid labels\n",
    "    test_labels_raw = pd.read_csv('data/test_labels.csv.zip', compression='zip').values.tolist()\n",
    "    test_labels = [[float(j) for j in i[1:]] for i in test_labels_raw]\n",
    "    indices = [i for i in range(len(test_labels)) if test_labels[i][0] != -1]\n",
    "    test_labels = [test_labels[i] for i in indices]\n",
    "    test_data_raw = pd.read_csv('data/test.csv.zip', compression='zip').values.tolist()\n",
    "    test_data = [clean_str(i[1]).split(\" \") for i in test_data_raw]\n",
    "    test_data = [test_data[i] for i in indices]\n",
    "    \n",
    "    # We build our train, dev and test dataset and return them\n",
    "    if label==-1:\n",
    "        train_dataset = (train_data[:100000], train_labels[:100000])\n",
    "        dev_dataset = (train_data[152000:153000], train_labels[152000:153000])\n",
    "        test_dataset = (test_data[:2000], test_labels[:2000])\n",
    "    else:\n",
    "        train_dataset = (train_data[:100000], [[i[label]] for i in train_labels[:100000]])\n",
    "        dev_dataset = (train_data[152000:153000], [[i[label]] for i in train_labels[152000:153000]])\n",
    "        test_dataset = (test_data[:2000], [[i[label]] for i in test_labels[:2000]])    \n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load_data()\n",
    "# For some losses, we can weight differently the classes\n",
    "# This gives us the weight we should give to the positive class for each label\n",
    "for j in range(len(train_dataset[1][0])):\n",
    "    weight = sum(i[j] for i in train_dataset[1])\n",
    "    print(\"Weight of label\",j,\"is\",(100000-weight)/weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "# If tokenize is true, we use a tokenizer to normalize the words\n",
    "# Otherwise, we leave them as they are\n",
    "def build_dictionary(train_data, tokenize=False):\n",
    "    if tokenize:\n",
    "        tokenizer = text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(list(train_data))\n",
    "        tokenizer.word_index[\"<PAD>\"] = len(tokenizer.word_index.keys())\n",
    "        padding_idx = tokenizer.word_index[\"<PAD>\"]\n",
    "        identifiers = tokenizer.word_index\n",
    "    else : \n",
    "        identifiers = {}\n",
    "        for comment in train_data:\n",
    "            for word in comment:\n",
    "                if word not in identifiers.keys():\n",
    "                    identifiers[word] = len(identifiers.keys())\n",
    "        identifiers[\"<PAD>\"] = len(identifiers.keys())\n",
    "        padding_idx = identifiers[\"<PAD>\"]\n",
    "    return identifiers, padding_idx\n",
    "        \n",
    "identifiers, padding_idx = build_dictionary(train_dataset[0], tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming our dataset\n",
    "We replace the words by their identifiers if they exist and add padding so that all the sentences in a given dataset have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function replaces each word by its index in the dictionary if it exists\n",
    "# If not, the word is deleted\n",
    "# Then, we pad each example so that they all have the same size of at least 1\n",
    "def transform_data(dataset):\n",
    "    tmp = [[identifiers[j] for j in i if j in identifiers.keys()] for i in dataset[0]]\n",
    "    m = max(len(i) for i in tmp)\n",
    "    tmp = [i+(m-len(i)+1)*[identifiers[\"<PAD>\"]] for i in tmp]\n",
    "    return (tmp, dataset[1])\n",
    "\n",
    "train_dataset = transform_data(train_dataset)\n",
    "dev_dataset = transform_data(dev_dataset)\n",
    "test_dataset = transform_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and running the model\n",
    "We now build the dataset and model classes from our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build our dataset and delete the original data once it's done to save memory\n",
    "dataset = fk.Dataset(name=\"toxic\", train_data=train_dataset[0], train_labels=train_dataset[1], \n",
    "                                   dev_data=dev_dataset[0], dev_labels=dev_dataset[1],\n",
    "                                   test_data=test_dataset[0], test_labels=test_dataset[1], \n",
    "                                   data_type='long', label_type='long', batch_size=128)\n",
    "del train_dataset; del dev_dataset; del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model : LSTM_classifier / loss : L.Loss_1LSTM_1Linear6 / metric : M.BAC_6_classes_1_out\n",
    "# model : LSTM_classifier_1LSTM / loss : L.Loss_1LSTM_6Linear / metric : M.BAC_6_classes_2_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the model in our framework and force the garbage collector to clear memory\n",
    "model = LSTM_classifier(len(identifiers.keys()), 20, 20, 20)\n",
    "optimizer = optim.Adagrad(model.parameters(),lr=0.1, lr_decay = 0.01)\n",
    "loss = L.Loss_1LSTM_1Linear6()\n",
    "metric = M.BAC_6_classes_1_out()\n",
    "encaps = fk.Model(name=\"one_model\", model=model, loss=loss, optimizer=optimizer, metric=metric, dataset=dataset)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encaps.train(epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encaps.restore_best()\n",
    "encaps.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40 0.608\n",
    "30 0.636\n",
    "20 0.636\n",
    "\n",
    "10 20 0.629\n",
    "30 20 0.624"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
